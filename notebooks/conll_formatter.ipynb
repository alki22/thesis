{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sh\n",
    "\n",
    "from collections import Counter\n",
    "from lxml import etree\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "base_directory = './word_sense_disambigation_corpora/'\n",
    "sentences = 1\n",
    "\n",
    "with open('google_wsd.col', 'w') as fout:\n",
    "    for file in tqdm_notebook(sh.find('%smasc' % base_directory, '%ssemcor' % base_directory,\n",
    "                                      '-type', 'f', '-name', '*.xml')):\n",
    "        root = etree.parse(file.strip()).getroot()\n",
    "\n",
    "        sentence = []\n",
    "        verb_senses = []\n",
    "        for word in root.findall('word'):\n",
    "            if word.attrib['text'].strip() == '':\n",
    "                continue\n",
    "            \n",
    "            break_level = word.attrib['break_level']\n",
    "            \n",
    "            docname = file.strip().replace(base_directory, '')\n",
    "            corpus = 'masc' if docname.startswith('masc') else 'semcor'\n",
    "            domain = 'semcor' if corpus == 'semcor' else os.path.dirname(docname)[5:]\n",
    "\n",
    "            if break_level == 'PARAGRAPH_BREAK' or break_level == 'SENTENCE_BREAK':\n",
    "                for vidx, token, lemma, sense in verb_senses:\n",
    "                    meta_string = 'META:%s\\tsentence:%05d\\t' % (corpus, sentences)\n",
    "                    meta_string += 'doc:%s\\t' % docname\n",
    "                    meta_string += 'domain:%s\\t' % domain\n",
    "                    meta_string += 'main_lemma:%s\\t' % lemma\n",
    "                    meta_string += 'main_lemma_index:%d\\t' % vidx\n",
    "                    meta_string += 'main_token:%s\\t' % token\n",
    "                    meta_string += 'sense:%s' % sense\n",
    "                    print(meta_string, file=fout)\n",
    "                    print('\\n'.join(sentence), file=fout, end='\\n\\n')\n",
    "                    sentences += 1\n",
    "                sentence = []\n",
    "                verb_senses = []\n",
    "\n",
    "            sidx = len(sentence) + 1\n",
    "            if 'pos' in word.attrib and word.attrib['pos'] == 'VERB':\n",
    "                verb_senses.append((sidx, word.attrib['text'], word.attrib['lemma'], word.attrib['sense']))\n",
    "                sentence.append('%d\\t%s\\t%s\\t-' % (sidx, word.attrib['text'], word.attrib['lemma']))\n",
    "            else:\n",
    "                sentence.append('%d\\t%s\\t-\\t-' % (sidx, word.attrib['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "last_meta = {}\n",
    "with open('./google_wsd.conll', 'r') as fin:\n",
    "    for line in fin:\n",
    "        if line.startswith('META'):\n",
    "            last_meta = dict(w.split(':', 1) for w in line.strip().split())\n",
    "            last_meta['sense'] = last_meta['sense'].split('/')[-1]\n",
    "        try:\n",
    "            if line.strip().split()[0] == last_meta['main_lemma_index']:\n",
    "                last_meta['correctly_lemmatized'] = last_meta['main_lemma'] == line.strip().split()[2]\n",
    "                sentences.append(last_meta)\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "sentences = pd.DataFrame(sentences, columns=['META', 'sentence', 'doc', 'domain', 'main_lemma',\n",
    "                                             'main_lemma_index', 'main_token', 'sense', 'correctly_lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences['domain_sentence_count'] = sentences\\\n",
    "    .groupby(['main_lemma', 'sense', 'domain'])['sentence'].transform('count')\n",
    "\n",
    "sentences['sense_sentence_count'] = sentences\\\n",
    "    .groupby(['main_lemma', 'sense'])['sentence'].transform('count')\n",
    "\n",
    "sentences['lemma_sentence_count'] = sentences\\\n",
    "    .groupby(['main_lemma'])['sentence'].transform('count')\n",
    "    \n",
    "sentences['sense_count'] = sentences\\\n",
    "    .groupby(['main_lemma'])['sense']\\\n",
    "    .transform(lambda x: x.nunique())\n",
    "\n",
    "sentences['senses_over_threshold'] = sentences['main_lemma']\\\n",
    "    .map(sentences.groupby('main_lemma')\\\n",
    "    .apply(lambda x: x.loc[x.sense_sentence_count >= 3, 'sense'].nunique()))\n",
    "\n",
    "sentences['is_valid'] = (sentences['senses_over_threshold'] > 1) & (sentences['sense_sentence_count'] >= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences[sentences.is_valid].groupby(['main_lemma', 'sense']).first()['sense_sentence_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
