{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sh\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_file = '../../resources/sensem.conll'\n",
    "\n",
    "metadata_cols = ['META', 'sentence', 'corpus', 'main_lemma', 'main_lemma_index',\n",
    "                 'resource_sentence', 'sense', 'wn', 'wn16', 'wn30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_file = '../../resources/corpora/semeval.conll'\n",
    "\n",
    "metadata_cols = ['META', 'sentence', 'corpus', 'doc', 'lemma_tag', 'main_lemma',\n",
    "                 'main_lemma_index', 'resource_sentence', 'sense']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metadata = []\n",
    "for mdata in sh.grep('^META', input_file):\n",
    "    metadata.append(dict(md.split(':', 1) for md in mdata.strip().split()))\n",
    "\n",
    "metadata = pd.DataFrame(metadata, columns=metadata_cols)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered = metadata.groupby([lemma_column, sense_column]).filter(\n",
    "    lambda x: len(x) < 2 or x[sense_column].values[0] == '-').index\n",
    "metadata.loc[filtered,'corpus'] = 'filtered'\n",
    "\n",
    "non_filtered = metadata.groupby([lemma_column, sense_column]).filter(\n",
    "    lambda x: len(x) >= 2 and x[sense_column].values[0] != '-').index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "\n",
    "labels = metadata.loc[non_filtered][sense_column]\n",
    "\n",
    "classes, y_counts = np.unique(labels, return_counts=True)\n",
    "n_cls = classes.shape[0]\n",
    "n_test = labels.shape[0] * test_size\n",
    "n_train = labels.shape[0] - n_test\n",
    "\n",
    "assert n_train >= n_cls and n_test >= n_cls\n",
    "\n",
    "test_count = np.maximum(np.round(y_counts * test_size), np.ones(n_cls)).astype(np.int32)\n",
    "train_count = (y_counts - test_count).astype(np.int32)\n",
    "\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "for idx, cls in enumerate(classes):\n",
    "    labels_for_class = labels[labels == cls]\n",
    "\n",
    "    train_indices.extend(labels_for_class[:train_count[idx]].index)\n",
    "    test_indices.extend(labels_for_class[train_count[idx]:train_count[idx]+test_count[idx]].index)\n",
    "\n",
    "train_indices = np.array(train_indices, dtype=np.int32)\n",
    "test_indices = np.array(test_indices, dtype=np.int32)\n",
    "\n",
    "metadata.loc[train_indices, 'corpus'] = 'train'\n",
    "metadata.loc[test_indices, 'corpus'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta_lines = ('\\t'.join(\":\".join(r) for r in zip(row.index, row)) for _, row in metadata.iterrows())\n",
    "\n",
    "with open(input_file, 'r') as fin, open(output_file, 'w') as fout:\n",
    "    for line in tqdm_notebook(fin, total=840705):\n",
    "        if line.startswith(\"META\"):\n",
    "            print(next(meta_lines), file=fout)\n",
    "        else:\n",
    "            print(line.strip(), file=fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train/Test split for Google WSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_file = '../../resources/corpora/google_wsd.conll'\n",
    "output_file = '../../resources/corpora/google_wsd.conll.new'\n",
    "\n",
    "sentences = []\n",
    "last_meta = {}\n",
    "with open('../../resources/corpora/google_wsd.conll', 'r') as fin:\n",
    "    for line in fin:\n",
    "        if line.startswith('META'):\n",
    "            last_meta = dict(w.split(':', 1) for w in line.strip().split())\n",
    "            last_meta['sense'] = last_meta['sense'].split('/')[-1]\n",
    "        try:\n",
    "            if line.strip().split()[0] == last_meta['main_lemma_index']:\n",
    "                last_meta['correctly_lemmatized'] = last_meta['main_lemma'] == line.strip().split()[2]\n",
    "                sentences.append(last_meta)\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "sentences = pd.DataFrame(sentences, columns=['META', 'sentence', 'doc', 'domain', 'main_lemma',\n",
    "                                             'main_lemma_index', 'main_token', 'sense', 'correctly_lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences['domain_sentence_count'] = sentences\\\n",
    "    .groupby(['main_lemma', 'sense', 'domain'])['sentence'].transform('count')\n",
    "\n",
    "sentences['sense_sentence_count'] = sentences\\\n",
    "    .groupby(['main_lemma', 'sense'])['sentence'].transform('count')\n",
    "\n",
    "sentences['lemma_sentence_count'] = sentences\\\n",
    "    .groupby(['main_lemma'])['sentence'].transform('count')\n",
    "    \n",
    "sentences['sense_count'] = sentences\\\n",
    "    .groupby(['main_lemma'])['sense']\\\n",
    "    .transform(lambda x: x.nunique())\n",
    "\n",
    "sentences['senses_over_threshold'] = sentences['main_lemma']\\\n",
    "    .map(sentences.groupby('main_lemma')\\\n",
    "    .apply(lambda x: x.loc[x.sense_sentence_count >= 3, 'sense'].nunique()))\n",
    "\n",
    "sentences['is_valid'] = (sentences['senses_over_threshold'] > 1) & (sentences['sense_sentence_count'] >= 3)\n",
    "\n",
    "sentences.insert(2, 'corpus', 'filtered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "\n",
    "labels = sentences.loc[sentences.is_valid, 'sense']\n",
    "\n",
    "classes, y_counts = np.unique(labels, return_counts=True)\n",
    "n_cls = classes.shape[0]\n",
    "n_test = labels.shape[0] * test_size\n",
    "n_train = labels.shape[0] - n_test\n",
    "\n",
    "assert n_train >= n_cls and n_test >= n_cls\n",
    "\n",
    "test_count = np.maximum(np.round(y_counts * test_size), np.ones(n_cls)).astype(np.int32)\n",
    "train_count = (y_counts - test_count).astype(np.int32)\n",
    "\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "for idx, cls in enumerate(classes):\n",
    "    labels_for_class = labels[labels == cls]\n",
    "\n",
    "    train_indices.extend(labels_for_class[:train_count[idx]].index)\n",
    "    test_indices.extend(labels_for_class[train_count[idx]:train_count[idx]+test_count[idx]].index)\n",
    "\n",
    "train_indices = np.array(train_indices, dtype=np.int32)\n",
    "test_indices = np.array(test_indices, dtype=np.int32)\n",
    "\n",
    "sentences.loc[train_indices, 'corpus'] = 'train'\n",
    "sentences.loc[test_indices, 'corpus'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta_lines = ('\\t'.join(\":\".join(r) for r in zip(row.index, row.astype(str)))\n",
    "              for _, row in sentences.iloc[:, :10].iterrows())\n",
    "\n",
    "with open(input_file, 'r') as fin, open(output_file, 'w') as fout:\n",
    "    for line in tqdm_notebook(fin):\n",
    "        if line.startswith(\"META\"):\n",
    "            print(next(meta_lines), file=fout)\n",
    "        else:\n",
    "            print(line.strip(), file=fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
